{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Hackathon you are going to benchmark text classifiers w.r.t. each other. You can use the IMDB movie review data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise \n",
    "\n",
    "Make a plot of accuracy as a function of the number of data points in the training set for the models below. \n",
    "\n",
    "Models to compare: All seperate and share tips/repo/questions\n",
    "* Make a baseline model (simple model, from e.g. sklearn) All\n",
    "* Make an RNN based model and fit the word embeddings yourself\n",
    "* Make an RNN based model, but use word embeddings from word2vec or document tensor from Spacy (see hint below) as input (i.e. do not start with an embedding layer!)\n",
    "* Use a pretrained BertModel from HuggingFace, only fit the classifier layers\n",
    "* Fit a sklearn model on the pooled output of a pretrained BERT model\n",
    "\n",
    "Compute intensive jobs (Bonus): All: together\n",
    "* Fine-tune a pretrained BertModel (i.e. fit all layers, might need a GPU for this)\n",
    "* Re-initialize the weights of the BERT model and fit from scratch \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hint: Spacy document tensors\n",
    "\n",
    "Feeding a Spacy document tensor into an RNN can yield very nice results. Spacy is part of the conda environment, and should already be installed. Make sure that you download at least the medium size english model ([taken from the quickstart from the Spacy docs](https://spacy.io/usage#installation))\n",
    "\n",
    "```bash\n",
    "python -m spacy download en_core_web_md\n",
    "```\n",
    "\n",
    "Example code for converting to tensors:\n",
    "```python\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "tensorized = doc.tensor\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hint: Reducing compute time\n",
    "\n",
    "Try to cache (memory or disk) if processing takes too long. E.g. word-to-vec operations. Also, the BERT output can be cached."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
