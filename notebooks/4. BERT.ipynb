{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained BERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a pretrained BertModel from HuggingFace, only fit the classifier layers\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/master/notebooks/02-transformers.ipynb\n",
    "\n",
    "Download distilbert model:\n",
    "* https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-tf_model.h5\n",
    "* https://s3.amazonaws.com/models.huggingface.co/bert/distilbert-base-cased-config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T05:58:53.809133Z",
     "start_time": "2020-04-17T05:58:50.107055Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import re\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2000 records is 3 minutes for creating the embeddings. If we assume linear performance it would take 75 minutes to convert all embeddings. Unfortantely, it leads to a dead kernel in the tokenize step. We need to create batches to run this on a local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:02:06.837722Z",
     "start_time": "2020-04-17T06:02:06.266209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/IMDB Dataset.csv')\n",
    "\n",
    "SAMPLE_SIZE = 50000\n",
    "\n",
    "def preprocess_imdb_raw_data(x):\n",
    "    x = re.sub(\"<br\\\\s*/?>\", \" \", x)\n",
    "    return x \n",
    "\n",
    "X = [preprocess_imdb_raw_data(x) for x in df['review'].values][:SAMPLE_SIZE]\n",
    "\n",
    "y = df['sentiment'].apply(lambda x: int(x == 'positive')).values[:SAMPLE_SIZE]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a transformers pipeline\n",
    "Without any additional training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:25:15.564023Z",
     "start_time": "2020-04-17T06:25:11.693804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8988238e95e49a9841b9078b4f90b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=230.0, style=ProgressStyle(description_â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp_sentence_classif = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:25:36.883713Z",
     "start_time": "2020-04-17T06:25:36.853822Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use the same test set as before\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:57:02.712248Z",
     "start_time": "2020-04-17T06:25:41.454262Z"
    }
   },
   "outputs": [],
   "source": [
    "predicted_sentiment = [nlp_sentence_classif(x)[0]['label'].lower() for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:57:07.653210Z",
     "start_time": "2020-04-17T06:57:07.633256Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89      5044\n",
      "           1       0.91      0.86      0.88      4956\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = [s == 'positive' for s in predicted_sentiment]\n",
    "\n",
    "print(f\"Test: {classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T18:27:49.478363Z",
     "start_time": "2020-04-16T18:27:49.474261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x142171080>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "from transformers import TFBertModel, BertModel\n",
    "\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Can you use the tokenizer from a different model?\n",
    "\n",
    "Q: Distilbert also takes around 3 to create embeddings. What is the efficiency gain that we could have expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T18:28:07.448643Z",
     "start_time": "2020-04-16T18:28:04.318717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Store the model we want to use\n",
    "MODEL_NAME = \"bert-base-cased\" \n",
    "\n",
    "# We need to create the model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# MODEL_NAME = \"../models/distilbert-base-cased\"\n",
    "\n",
    "model_pt = BertModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model_pt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T05:57:51.643250Z",
     "start_time": "2020-04-16T18:40:06.544415Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 200\n",
    "\n",
    "def get_sentence_embedding(text):\n",
    "    \n",
    "    tokenized_text = tokenizer.encode(text, \n",
    "                                      return_tensors='pt', \n",
    "                                      max_length=MAX_SEQ_LENGTH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states, _ = model_pt(tokenized_text, )\n",
    "    \n",
    "    # Use the mean of the sentence embeddings\n",
    "    return torch.mean(last_hidden_states, dim=1).numpy().reshape(1, -1)[0]\n",
    "\n",
    "embeddings = np.concatenate([\n",
    "  get_sentence_embedding(text)\n",
    "  for text in X\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sentence embeddings per batch\n",
    "Contains error - no proper results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T14:03:38.039283Z",
     "start_time": "2020-04-14T14:03:38.035815Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_embeddings(X, batch_size):\n",
    "    \n",
    "    for i in range(0, len(X), batch_size):\n",
    "    \n",
    "        tokens = tokenizer.batch_encode_plus(X[i:i+batch_size], \n",
    "                                         max_length=MAX_SEQ_LENGTH, \n",
    "                                         pad_to_max_length=True,\n",
    "                                         return_tensors='tf')\n",
    "        _, pooled = model_tf(tokens)\n",
    "\n",
    "        yield i, np.array(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-04-14T14:04:00.378Z"
    }
   },
   "outputs": [],
   "source": [
    "for (i, embedding) in generate_embeddings(X, 5000):\n",
    "    np.save(f'../models/bert-model/bert_pooled_layer_{i}.npy', embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:02:54.990871Z",
     "start_time": "2020-04-17T07:02:54.784951Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "embeddings = np.concatenate([\n",
    "    np.load(f'../models/bert-model/bert_pooled_layer_{i}.npy')\n",
    "    for i in range(0, SAMPLE_SIZE, 5000)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:02:59.688289Z",
     "start_time": "2020-04-17T07:02:59.481845Z"
    }
   },
   "outputs": [],
   "source": [
    "normalized_embeddings = Normalizer().fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:03:04.474203Z",
     "start_time": "2020-04-17T07:03:04.378824Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(normalized_embeddings, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for last clf layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:04:17.443228Z",
     "start_time": "2020-04-17T07:04:17.386505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 768)]             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               98432     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 98,561\n",
      "Trainable params: 98,561\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def make_simple_model(embedding_size=768):\n",
    "\n",
    "    inp = Input(shape=[embedding_size])\n",
    "    \n",
    "    x = Dense(128, activation=\"relu\")(inp)\n",
    "    \n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inp, out)\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    model.compile(Adam(lr=5e-3), loss=losses.binary_crossentropy, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_clf = make_simple_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:08:05.277530Z",
     "start_time": "2020-04-17T07:04:29.715973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples\n",
      "Epoch 1/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.6710 - accuracy: 0.5826\n",
      "Epoch 2/150\n",
      "40000/40000 [==============================] - 1s 33us/sample - loss: 0.6445 - accuracy: 0.6234\n",
      "Epoch 3/150\n",
      "40000/40000 [==============================] - 1s 32us/sample - loss: 0.6269 - accuracy: 0.6482\n",
      "Epoch 4/150\n",
      "40000/40000 [==============================] - 1s 34us/sample - loss: 0.6171 - accuracy: 0.6563\n",
      "Epoch 5/150\n",
      "40000/40000 [==============================] - 1s 34us/sample - loss: 0.6073 - accuracy: 0.6685\n",
      "Epoch 6/150\n",
      "40000/40000 [==============================] - 1s 34us/sample - loss: 0.6076 - accuracy: 0.6680\n",
      "Epoch 7/150\n",
      "40000/40000 [==============================] - 1s 35us/sample - loss: 0.6052 - accuracy: 0.6719\n",
      "Epoch 8/150\n",
      "40000/40000 [==============================] - 1s 34us/sample - loss: 0.5996 - accuracy: 0.6759\n",
      "Epoch 9/150\n",
      "40000/40000 [==============================] - 1s 37us/sample - loss: 0.5976 - accuracy: 0.6800\n",
      "Epoch 10/150\n",
      "40000/40000 [==============================] - 1s 35us/sample - loss: 0.5927 - accuracy: 0.6812\n",
      "Epoch 11/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5912 - accuracy: 0.6834\n",
      "Epoch 12/150\n",
      "40000/40000 [==============================] - 1s 34us/sample - loss: 0.5898 - accuracy: 0.6846\n",
      "Epoch 13/150\n",
      "40000/40000 [==============================] - 2s 38us/sample - loss: 0.5923 - accuracy: 0.6813\n",
      "Epoch 14/150\n",
      "40000/40000 [==============================] - 2s 44us/sample - loss: 0.5839 - accuracy: 0.6903\n",
      "Epoch 15/150\n",
      "40000/40000 [==============================] - 2s 38us/sample - loss: 0.5851 - accuracy: 0.6872\n",
      "Epoch 16/150\n",
      "40000/40000 [==============================] - 1s 35us/sample - loss: 0.5834 - accuracy: 0.6898\n",
      "Epoch 17/150\n",
      "40000/40000 [==============================] - 2s 38us/sample - loss: 0.5818 - accuracy: 0.6915\n",
      "Epoch 18/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5822 - accuracy: 0.6910\n",
      "Epoch 19/150\n",
      "40000/40000 [==============================] - 1s 35us/sample - loss: 0.5847 - accuracy: 0.6895\n",
      "Epoch 20/150\n",
      "40000/40000 [==============================] - 2s 38us/sample - loss: 0.5798 - accuracy: 0.6923\n",
      "Epoch 21/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5787 - accuracy: 0.6961\n",
      "Epoch 22/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5761 - accuracy: 0.6980\n",
      "Epoch 23/150\n",
      "40000/40000 [==============================] - 2s 42us/sample - loss: 0.5753 - accuracy: 0.6965\n",
      "Epoch 24/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5766 - accuracy: 0.6943\n",
      "Epoch 25/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5745 - accuracy: 0.6952\n",
      "Epoch 26/150\n",
      "40000/40000 [==============================] - 2s 42us/sample - loss: 0.5722 - accuracy: 0.7001\n",
      "Epoch 27/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5721 - accuracy: 0.6975\n",
      "Epoch 28/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5723 - accuracy: 0.6981\n",
      "Epoch 29/150\n",
      "40000/40000 [==============================] - 2s 42us/sample - loss: 0.5713 - accuracy: 0.7002\n",
      "Epoch 30/150\n",
      "40000/40000 [==============================] - 2s 41us/sample - loss: 0.5754 - accuracy: 0.6968\n",
      "Epoch 31/150\n",
      "40000/40000 [==============================] - 2s 40us/sample - loss: 0.5687 - accuracy: 0.7020\n",
      "Epoch 32/150\n",
      "40000/40000 [==============================] - 2s 40us/sample - loss: 0.5677 - accuracy: 0.7026\n",
      "Epoch 33/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5684 - accuracy: 0.7028\n",
      "Epoch 34/150\n",
      "40000/40000 [==============================] - 2s 41us/sample - loss: 0.5667 - accuracy: 0.7036\n",
      "Epoch 35/150\n",
      "40000/40000 [==============================] - 2s 41us/sample - loss: 0.5692 - accuracy: 0.7003\n",
      "Epoch 36/150\n",
      "40000/40000 [==============================] - 2s 41us/sample - loss: 0.5679 - accuracy: 0.7016\n",
      "Epoch 37/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5670 - accuracy: 0.7020\n",
      "Epoch 38/150\n",
      "40000/40000 [==============================] - 2s 40us/sample - loss: 0.5682 - accuracy: 0.7015\n",
      "Epoch 39/150\n",
      "40000/40000 [==============================] - 2s 38us/sample - loss: 0.5654 - accuracy: 0.7046\n",
      "Epoch 40/150\n",
      "40000/40000 [==============================] - 2s 40us/sample - loss: 0.5688 - accuracy: 0.7008\n",
      "Epoch 41/150\n",
      "40000/40000 [==============================] - 2s 43us/sample - loss: 0.5651 - accuracy: 0.7056\n",
      "Epoch 42/150\n",
      "40000/40000 [==============================] - 2s 41us/sample - loss: 0.5643 - accuracy: 0.7054\n",
      "Epoch 43/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5653 - accuracy: 0.7031\n",
      "Epoch 44/150\n",
      "40000/40000 [==============================] - 2s 44us/sample - loss: 0.5667 - accuracy: 0.7019\n",
      "Epoch 45/150\n",
      "40000/40000 [==============================] - 2s 42us/sample - loss: 0.5637 - accuracy: 0.7046\n",
      "Epoch 46/150\n",
      "40000/40000 [==============================] - 2s 40us/sample - loss: 0.5646 - accuracy: 0.7031\n",
      "Epoch 47/150\n",
      "40000/40000 [==============================] - 2s 41us/sample - loss: 0.5615 - accuracy: 0.7075\n",
      "Epoch 48/150\n",
      "40000/40000 [==============================] - 2s 41us/sample - loss: 0.5641 - accuracy: 0.7055\n",
      "Epoch 49/150\n",
      "40000/40000 [==============================] - 1s 37us/sample - loss: 0.5618 - accuracy: 0.7053\n",
      "Epoch 50/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5610 - accuracy: 0.7074\n",
      "Epoch 51/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5611 - accuracy: 0.7068\n",
      "Epoch 52/150\n",
      "40000/40000 [==============================] - 2s 40us/sample - loss: 0.5619 - accuracy: 0.7045\n",
      "Epoch 53/150\n",
      "40000/40000 [==============================] - 2s 38us/sample - loss: 0.5618 - accuracy: 0.7064\n",
      "Epoch 54/150\n",
      "40000/40000 [==============================] - 2s 40us/sample - loss: 0.5576 - accuracy: 0.7096\n",
      "Epoch 55/150\n",
      "40000/40000 [==============================] - 2s 42us/sample - loss: 0.5611 - accuracy: 0.7066s - loss:\n",
      "Epoch 56/150\n",
      "40000/40000 [==============================] - 2s 42us/sample - loss: 0.5601 - accuracy: 0.7064\n",
      "Epoch 57/150\n",
      "40000/40000 [==============================] - 2s 41us/sample - loss: 0.5584 - accuracy: 0.7091\n",
      "Epoch 58/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5583 - accuracy: 0.7099\n",
      "Epoch 59/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5600 - accuracy: 0.7085\n",
      "Epoch 60/150\n",
      "40000/40000 [==============================] - 1s 37us/sample - loss: 0.5554 - accuracy: 0.7108\n",
      "Epoch 61/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5589 - accuracy: 0.7085\n",
      "Epoch 62/150\n",
      "40000/40000 [==============================] - 1s 37us/sample - loss: 0.5576 - accuracy: 0.7099\n",
      "Epoch 63/150\n",
      "40000/40000 [==============================] - 2s 44us/sample - loss: 0.5594 - accuracy: 0.7069\n",
      "Epoch 64/150\n",
      "40000/40000 [==============================] - 2s 45us/sample - loss: 0.5562 - accuracy: 0.7102\n",
      "Epoch 65/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5559 - accuracy: 0.7103\n",
      "Epoch 66/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5570 - accuracy: 0.7109\n",
      "Epoch 67/150\n",
      "40000/40000 [==============================] - 2s 40us/sample - loss: 0.5572 - accuracy: 0.7100\n",
      "Epoch 68/150\n",
      "40000/40000 [==============================] - 2s 38us/sample - loss: 0.5551 - accuracy: 0.7102\n",
      "Epoch 69/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5559 - accuracy: 0.7121\n",
      "Epoch 70/150\n",
      "40000/40000 [==============================] - 2s 41us/sample - loss: 0.5550 - accuracy: 0.7135\n",
      "Epoch 71/150\n",
      "40000/40000 [==============================] - 2s 41us/sample - loss: 0.5558 - accuracy: 0.7107\n",
      "Epoch 72/150\n",
      "40000/40000 [==============================] - 2s 44us/sample - loss: 0.5547 - accuracy: 0.7106\n",
      "Epoch 73/150\n",
      "40000/40000 [==============================] - 2s 46us/sample - loss: 0.5557 - accuracy: 0.7116\n",
      "Epoch 74/150\n",
      "40000/40000 [==============================] - 2s 45us/sample - loss: 0.5558 - accuracy: 0.7094\n",
      "Epoch 75/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 2s 44us/sample - loss: 0.5568 - accuracy: 0.7091s - loss: 0.554\n",
      "Epoch 76/150\n",
      "40000/40000 [==============================] - 1s 25us/sample - loss: 0.5517 - accuracy: 0.7134\n",
      "Epoch 77/150\n",
      "40000/40000 [==============================] - 2s 40us/sample - loss: 0.5537 - accuracy: 0.7139\n",
      "Epoch 78/150\n",
      "40000/40000 [==============================] - 2s 43us/sample - loss: 0.5537 - accuracy: 0.7130\n",
      "Epoch 79/150\n",
      "40000/40000 [==============================] - 2s 46us/sample - loss: 0.5535 - accuracy: 0.7092\n",
      "Epoch 80/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5521 - accuracy: 0.7144\n",
      "Epoch 81/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5526 - accuracy: 0.7138\n",
      "Epoch 82/150\n",
      "40000/40000 [==============================] - 1s 35us/sample - loss: 0.5520 - accuracy: 0.7139\n",
      "Epoch 83/150\n",
      "40000/40000 [==============================] - 1s 37us/sample - loss: 0.5540 - accuracy: 0.7149\n",
      "Epoch 84/150\n",
      "40000/40000 [==============================] - 2s 43us/sample - loss: 0.5515 - accuracy: 0.7132\n",
      "Epoch 85/150\n",
      "40000/40000 [==============================] - 1s 32us/sample - loss: 0.5524 - accuracy: 0.7127\n",
      "Epoch 86/150\n",
      "40000/40000 [==============================] - 1s 33us/sample - loss: 0.5505 - accuracy: 0.7146\n",
      "Epoch 87/150\n",
      "40000/40000 [==============================] - 1s 34us/sample - loss: 0.5527 - accuracy: 0.7127\n",
      "Epoch 88/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5515 - accuracy: 0.7133\n",
      "Epoch 89/150\n",
      "40000/40000 [==============================] - 1s 34us/sample - loss: 0.5534 - accuracy: 0.7104\n",
      "Epoch 90/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5498 - accuracy: 0.7165\n",
      "Epoch 91/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5494 - accuracy: 0.7156\n",
      "Epoch 92/150\n",
      "40000/40000 [==============================] - 1s 33us/sample - loss: 0.5495 - accuracy: 0.7147\n",
      "Epoch 93/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5496 - accuracy: 0.7162\n",
      "Epoch 94/150\n",
      "40000/40000 [==============================] - 1s 33us/sample - loss: 0.5517 - accuracy: 0.7152\n",
      "Epoch 95/150\n",
      "40000/40000 [==============================] - 1s 31us/sample - loss: 0.5492 - accuracy: 0.7172\n",
      "Epoch 96/150\n",
      "40000/40000 [==============================] - 1s 35us/sample - loss: 0.5480 - accuracy: 0.7164\n",
      "Epoch 97/150\n",
      "40000/40000 [==============================] - 1s 32us/sample - loss: 0.5498 - accuracy: 0.7168\n",
      "Epoch 98/150\n",
      "40000/40000 [==============================] - 1s 31us/sample - loss: 0.5481 - accuracy: 0.7156\n",
      "Epoch 99/150\n",
      "40000/40000 [==============================] - 1s 30us/sample - loss: 0.5461 - accuracy: 0.7200\n",
      "Epoch 100/150\n",
      "40000/40000 [==============================] - 1s 29us/sample - loss: 0.5474 - accuracy: 0.7167\n",
      "Epoch 101/150\n",
      "40000/40000 [==============================] - 1s 37us/sample - loss: 0.5487 - accuracy: 0.7145\n",
      "Epoch 102/150\n",
      "40000/40000 [==============================] - 1s 34us/sample - loss: 0.5470 - accuracy: 0.7189\n",
      "Epoch 103/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5486 - accuracy: 0.7155\n",
      "Epoch 104/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5463 - accuracy: 0.7190\n",
      "Epoch 105/150\n",
      "40000/40000 [==============================] - 1s 37us/sample - loss: 0.5457 - accuracy: 0.7170\n",
      "Epoch 106/150\n",
      "40000/40000 [==============================] - 1s 33us/sample - loss: 0.5469 - accuracy: 0.7170\n",
      "Epoch 107/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5495 - accuracy: 0.7152\n",
      "Epoch 108/150\n",
      "40000/40000 [==============================] - 1s 37us/sample - loss: 0.5472 - accuracy: 0.7172\n",
      "Epoch 109/150\n",
      "40000/40000 [==============================] - 1s 34us/sample - loss: 0.5466 - accuracy: 0.7182\n",
      "Epoch 110/150\n",
      "40000/40000 [==============================] - 1s 31us/sample - loss: 0.5458 - accuracy: 0.7178s - los\n",
      "Epoch 111/150\n",
      "40000/40000 [==============================] - 1s 31us/sample - loss: 0.5471 - accuracy: 0.7157\n",
      "Epoch 112/150\n",
      "40000/40000 [==============================] - 1s 28us/sample - loss: 0.5459 - accuracy: 0.7168\n",
      "Epoch 113/150\n",
      "40000/40000 [==============================] - 1s 32us/sample - loss: 0.5452 - accuracy: 0.7201\n",
      "Epoch 114/150\n",
      "40000/40000 [==============================] - 2s 42us/sample - loss: 0.5439 - accuracy: 0.7214\n",
      "Epoch 115/150\n",
      "40000/40000 [==============================] - 2s 41us/sample - loss: 0.5450 - accuracy: 0.7204\n",
      "Epoch 116/150\n",
      "40000/40000 [==============================] - 2s 38us/sample - loss: 0.5460 - accuracy: 0.7193\n",
      "Epoch 117/150\n",
      "40000/40000 [==============================] - 2s 39us/sample - loss: 0.5441 - accuracy: 0.7209\n",
      "Epoch 118/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5464 - accuracy: 0.7170\n",
      "Epoch 119/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5441 - accuracy: 0.7216\n",
      "Epoch 120/150\n",
      "40000/40000 [==============================] - 2s 43us/sample - loss: 0.5443 - accuracy: 0.7182\n",
      "Epoch 121/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5425 - accuracy: 0.7236\n",
      "Epoch 122/150\n",
      "40000/40000 [==============================] - 1s 36us/sample - loss: 0.5447 - accuracy: 0.7174\n",
      "Epoch 123/150\n",
      "40000/40000 [==============================] - 1s 32us/sample - loss: 0.5451 - accuracy: 0.7196\n",
      "Epoch 124/150\n",
      "40000/40000 [==============================] - 1s 28us/sample - loss: 0.5452 - accuracy: 0.7172\n",
      "Epoch 125/150\n",
      "40000/40000 [==============================] - 1s 29us/sample - loss: 0.5419 - accuracy: 0.7236s\n",
      "Epoch 126/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5451 - accuracy: 0.7178\n",
      "Epoch 127/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5433 - accuracy: 0.7201\n",
      "Epoch 128/150\n",
      "40000/40000 [==============================] - 1s 29us/sample - loss: 0.5423 - accuracy: 0.7228\n",
      "Epoch 129/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5436 - accuracy: 0.7198\n",
      "Epoch 130/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5414 - accuracy: 0.7213\n",
      "Epoch 131/150\n",
      "40000/40000 [==============================] - 1s 29us/sample - loss: 0.5415 - accuracy: 0.7208\n",
      "Epoch 132/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5425 - accuracy: 0.7209\n",
      "Epoch 133/150\n",
      "40000/40000 [==============================] - 1s 30us/sample - loss: 0.5403 - accuracy: 0.7224\n",
      "Epoch 134/150\n",
      "40000/40000 [==============================] - 1s 28us/sample - loss: 0.5431 - accuracy: 0.7211\n",
      "Epoch 135/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5420 - accuracy: 0.7214\n",
      "Epoch 136/150\n",
      "40000/40000 [==============================] - 1s 29us/sample - loss: 0.5434 - accuracy: 0.7217\n",
      "Epoch 137/150\n",
      "40000/40000 [==============================] - 1s 28us/sample - loss: 0.5398 - accuracy: 0.7236\n",
      "Epoch 138/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5416 - accuracy: 0.7230\n",
      "Epoch 139/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5408 - accuracy: 0.7224\n",
      "Epoch 140/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5409 - accuracy: 0.7223\n",
      "Epoch 141/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5399 - accuracy: 0.7228\n",
      "Epoch 142/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5427 - accuracy: 0.7216\n",
      "Epoch 143/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5397 - accuracy: 0.7244\n",
      "Epoch 144/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5395 - accuracy: 0.7216\n",
      "Epoch 145/150\n",
      "40000/40000 [==============================] - 1s 28us/sample - loss: 0.5410 - accuracy: 0.7214\n",
      "Epoch 146/150\n",
      "40000/40000 [==============================] - 1s 28us/sample - loss: 0.5402 - accuracy: 0.7231\n",
      "Epoch 147/150\n",
      "40000/40000 [==============================] - 1s 28us/sample - loss: 0.5403 - accuracy: 0.7212\n",
      "Epoch 148/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5403 - accuracy: 0.7229\n",
      "Epoch 149/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5376 - accuracy: 0.7247\n",
      "Epoch 150/150\n",
      "40000/40000 [==============================] - 1s 27us/sample - loss: 0.5432 - accuracy: 0.7167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a5252a58>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_clf.fit(X_train, y_train, epochs=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:11:00.169857Z",
     "start_time": "2020-04-17T07:10:59.949411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.53      0.63      5044\n",
      "           1       0.64      0.86      0.73      4956\n",
      "\n",
      "    accuracy                           0.69     10000\n",
      "   macro avg       0.72      0.69      0.68     10000\n",
      "weighted avg       0.72      0.69      0.68     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_test_probs = model_clf.predict(x=X_test)\n",
    "y_test_pred = (y_test_probs >= 0.5).astype(int)\n",
    "\n",
    "print(f\"Test: {classification_report(y_test, y_test_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T07:11:04.702537Z",
     "start_time": "2020-04-17T07:11:04.086281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.55      0.65     19956\n",
      "           1       0.66      0.87      0.75     20044\n",
      "\n",
      "    accuracy                           0.71     40000\n",
      "   macro avg       0.73      0.71      0.70     40000\n",
      "weighted avg       0.73      0.71      0.70     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_train_probs = model_clf.predict(x=X_train)\n",
    "y_train_pred = (y_train_probs >= 0.5).astype(int)\n",
    "\n",
    "print(f\"Train: {classification_report(y_train, y_train_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with own weights\n",
    "https://github.com/huggingface/transformers/issues/400#issuecomment-571354368\n",
    "\n",
    "~~~py\n",
    "for w in model.bert.weights():\n",
    "        w._trainable = False\n",
    "~~~\n",
    "\n",
    "Train on a GPU, use pytorch https://mccormickml.com/2019/07/22/BERT-fine-tuning/#11-using-colab-gpu-for-training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
